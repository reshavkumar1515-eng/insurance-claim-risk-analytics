# ğŸ›¡ï¸ Insurance Claim Risk Analytics
### Project 9 â€” End-to-End Data Engineering Project
**Wipro DAI-DATA Group 1 Training | ITER SOA | February 2026**

---

## ğŸ“Œ Project Overview

A complete, production-grade **Data Engineering & Analytics** solution for Insurance Claim Risk Analytics built on a modern data stack. The system ingests, transforms, and analyzes data across **1,000 customers**, **1,500 policies**, and **3,000 claims** â€” providing actionable fraud detection, risk scoring, and business intelligence insights.

---

## ğŸ—ï¸ Architecture (8-Layer Pipeline)

```
[Data Sources] â†’ [Python ETL] â†’ [Staging Layer] â†’ [Apache Spark]
      â†“                                                   â†“
[Snowflake DWH] â† [Star Schema DWH] â† [SQL Transforms]
      â†“
[RBAC + Masking + RLS] â†’ [Dashboard & BI Reports]
```

---

## ğŸ“‚ Folder Structure

```
insurance_claim_risk_analytics/
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ generate_data.py          # Synthetic data generator (1K customers, 3K claims)
â”‚
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ 01_star_schema_ddl.sql    # Star Schema DDL: dims, facts, indexes, partitions
â”‚   â””â”€â”€ 02_advanced_analytics_sql.sql  # CTEs, Window Fns, ROLLUP, CUBE, MERGE
â”‚
â”œâ”€â”€ ğŸ“ etl/
â”‚   â””â”€â”€ etl_pipeline.py           # Python ETL: Extract â†’ Transform â†’ Validate â†’ Load
â”‚
â”œâ”€â”€ ğŸ“ spark/
â”‚   â””â”€â”€ spark_processing.py       # Spark Batch + Structured Streaming
â”‚
â”œâ”€â”€ ğŸ“ snowflake/
â”‚   â””â”€â”€ snowflake_advanced.sql    # Clustering, Time Travel, VARIANT, RBAC, Snowpipe
â”‚
â”œâ”€â”€ ğŸ“ dashboard/
â”‚   â””â”€â”€ dashboard.py              # 2-page visualization dashboard (Matplotlib)
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ architecture_diagram.html # Interactive 8-layer architecture diagram
â”‚   â””â”€â”€ project_report.docx       # Full project report (11 sections)
â”‚
â”œâ”€â”€ ğŸ“ dashboard_output/          # Generated chart PNGs
â”‚   â”œâ”€â”€ dashboard_page1_overview.png
â”‚   â””â”€â”€ dashboard_page2_risk.png
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.10+ |
| Data Processing | Pandas, NumPy |
| Distributed Computing | Apache Spark 3.x (PySpark) |
| Streaming | Spark Structured Streaming + Kafka |
| Data Warehouse | PostgreSQL (Star Schema) + Snowflake |
| Visualization | Matplotlib |
| Orchestration | Apache Airflow (architecture) |
| Security | Snowflake RBAC, Data Masking, RLS |
| File Formats | CSV, JSON, Parquet, VARIANT |

---

## ğŸš€ How to Run

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/insurance-claim-risk-analytics.git
cd insurance-claim-risk-analytics
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Generate Sample Data
```bash
cd data
python generate_data.py
# Output: data/output/*.csv + claims_semi_structured.json
```

### 4. Run ETL Pipeline
```bash
python etl/etl_pipeline.py --source-dir data/output --output-dir etl_output
# Logs saved to: etl_pipeline.log
# Run stats saved to: etl_output/run_log.json
```

### 5. Run Spark Processing
```bash
# Requires PySpark installed
cd insurance_claim_risk_analytics
python spark/spark_processing.py
# Outputs: spark_output/claims_partitioned/*.parquet
```

### 6. Generate Dashboard
```bash
python dashboard/dashboard.py
# Charts saved to: dashboard_output/
```

### 7. SQL Scripts
- Run `sql/01_star_schema_ddl.sql` on PostgreSQL to create the Star Schema
- Run `sql/02_advanced_analytics_sql.sql` for advanced analytics queries
- Run `snowflake/snowflake_advanced.sql` on Snowflake for cloud DWH features

---

## ğŸ“Š Key Features Implemented

### âœ… Module 1 â€“ Data Warehouse Fundamentals
- OLTP vs OLAP design comparison
- ETL â†’ ELT pipeline transition
- Star Schema: 4 dimensions + 2 fact tables
- Staging â†’ Integration â†’ Presentation layers
- Metadata, data dictionary, lineage tracking

### âœ… Module 2 â€“ SQL L2 (Advanced)
- Partitioned tables (range partitioning by year)
- Indexing strategies (B-tree, partial, composite)
- CTEs (multi-level), Recursive CTEs
- Window Functions: RANK, LAG, LEAD, PERCENT_RANK, NTILE, Running SUM
- ROLLUP and CUBE for multi-dimensional aggregation
- MERGE (upsert) for SCD Type 1 updates
- EXPLAIN PLAN / EXPLAIN ANALYZE for query optimization
- Materialized Views with concurrent refresh

### âœ… Module 3 â€“ Python L2
- Decorators: `@timer`, `@retry`, `@audit_log`
- Custom exception hierarchy (`ETLException` â†’ `ExtractionError`, etc.)
- Lambda functions, list comprehensions, generators
- `argparse` CLI with `--source-dir`, `--output-dir`, `--log-level`
- CSV/JSON/Parquet file handling
- Pandas/NumPy for vectorized transformations
- PyODBC/SQLAlchemy-ready loader design

### âœ… Module 4 â€“ Apache Spark L2
- SparkSession with AQE, auto partition coalescing
- DataFrame API + Spark SQL (multi-join analytics)
- RDD: map, reduceByKey, cache, Accumulator, Broadcast variable
- Structured Streaming: file source, watermarks, tumbling window
- Fraud alert stream with alert_level classification
- Performance: broadcast joins, repartition, coalesce, Parquet output
- EXPLAIN (formatted physical plan)

### âœ… Module 5 â€“ Data Storytelling & Visualization
- 10 charts across 2 dashboard pages
- KPI summary cards
- Trend lines, bar charts, histograms, box plots
- Heatmap (Credit Ã— Income fraud rate)
- Bubble scatter plot (Region: amount vs fraud rate)
- Correct chart selection principles applied

### âœ… Module 6 â€“ Snowflake Advanced
- Virtual Warehouses (ETL_WH + ANALYTICS_WH) with auto-suspend
- Clustering keys with automatic re-clustering
- Time Travel: `AT(OFFSET)`, `BEFORE(STATEMENT)`, zero-copy clone
- VARIANT column for semi-structured JSON ingestion
- LATERAL FLATTEN for nested array querying
- Snowpipe: `AUTO_INGEST=TRUE` for event-driven loads
- COPY INTO (bulk load) + Parquet unload
- Materialized Views with auto-refresh
- RBAC: 4-tier role hierarchy
- Dynamic Data Masking: `annual_income`, `credit_score`
- Row-Level Security: region-based access policy
- Secure Data Sharing across accounts

---

## ğŸ“ˆ Results & Key Insights

| Metric | Value |
|---|---|
| Total Claims Processed | 3,000 |
| Fraud Detection Rate | ~12% |
| ETL Pipeline Runtime | 0.2 seconds |
| DQ Pass Rate | 100% |
| Star Schema Tables | 6 (4 dims + 2 facts) |
| SQL Features Demonstrated | 14+ |
| Dashboard Charts | 10 |
| Snowflake Features Used | 12+ |

---

## ğŸ“„ Project Report

Full documentation available in [`docs/project_report.docx`](docs/project_report.docx)

Interactive architecture diagram: [`docs/architecture_diagram.html`](docs/architecture_diagram.html)

---

## ğŸ‘¤ Author

**Student Name:** *Reshav Kumar Choudhary*  
**Batch:** Wipro DAI-DATA Group 1  
**Institute:** ITER, SOA University  
**Training Period:** February 2026  
**Project ID:** Project 9 â€” Insurance Claim Risk Analytics

---

> *Submitted as part of the Wipro DAI-DATA 30 End-to-End Data Engineering & Analytics Projects training program.*
